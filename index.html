<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description" content="a novel dataset specifically designed for the radiance field reconstruction task.">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;500&display=swap" rel="stylesheet">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="stylesheet" href="./static/css/bootstrap-4.4.1.css"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/preview.js"></script>
  <script src="./static/js/bottom.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->

  <section class="header">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D
              Reconstruction</h1>
            <hr>
            <div class="is-size-4 author-info">
              <span class="author-block">
                <a href="https://people.epfl.ch/zheng.dang" class="black-link">Zheng
                  Dang</a><sup>1</sup></span>&nbsp;&nbsp;
              <span class="author-block">
                Jialu Huang<sup>2</sup></span>&nbsp;&nbsp;
              <span class="author-block">
                Fei Wang<sup>2</sup>&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://people.epfl.ch/mathieu.salzmann" class="black-link">Mathieu Salzmann</a><sup>1</sup>
              </span>
            </div>

            <!-- <div class="is-size-5 author-info">
              <span class="author-block"><sup>1</sup>CVLab, EPFL, Switzerland</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Xi'an Jiaotong University, China</span>
            </div> -->

            <div class="is-size-5 author-info">
              <sup>1</sup> <a href="https://www.epfl.ch/"><img
                  style="height: 65px; padding-top: 18px; padding-bottom: 3pt;" src="static/images/logo/epfl-logo.svg">
              </a>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <sup>2</sup> <a href="http://www.xjtu.edu.cn/"><img
                  style="height: 65px; padding-top: 12px; padding-bottom: -4pt;" src="static/images/logo/logo-xjtu.png">
              </a>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="https://arxiv.org/abs/2406.08894" class="external-link button is-large is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <!-- Video Link. -->
                <!-- <span class="link-block"> -->
                <!-- <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark"> -->
                <!-- <a href="https://www.youtube.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <!-- <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="https://github.com/Christy61/OpenMaterial"
                    class="external-link button is-large is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/EPFL-CVLab/OpenMaterial"
                    class="external-link button is-large is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-full">
      <div class="hero-body">
        <img src="./static/images/teaser.png" style="width:75%; margin-left:auto; margin-right:auto; display:block">
        <div class="container is-max-desktop">
          <h2 class="subtitle is-max-desktop">
            <br>
            <b>Distinct materials.</b> From left to right: conductor, dielectric, plastic and diffuse. The top portion
            of the vase shows rough surface finishes, while the bottom one shows smooth surface finishes.
            <!-- <span class="dnerf"><b>OpenMaterial</b></span> Qualitative result of the material result. -->
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-max-desktop">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in deep learning such as neural radiance fields and implicit neural representations have
              significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with
              <b>complex optical properties</b>, such as metals and glass, remains a formidable challenge due to their
              unique specular and light-transmission characteristics. To facilitate the development of solutions to
              these
              challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct
              materials—including <b>conductors</b>, <b>dielectrics</b>, <b>plastics</b>, <b>and their roughened
                variants</b>— and captured
              under 723 diverse lighting conditions.
            </p>
            <p>
              To this end, we utilized <b>physics-based rendering with laboratory-measured Indices of
                Refraction(IOR)</b> and
              generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides
              comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask.
            </p>
            <p>
              It stands as the first <b>large-scale</b> dataset enabling quantitative evaluations of existing
              algorithms on objects with diverse and challenging materials, thereby paving the way for the development
              of 3D reconstruction algorithms capable of <b>handling complex material properties</b>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container-b">
      <div class="button-container">
        <div class="button-card" onclick="showContent('Preview')">
          <div class="title-b">Preview</div>
        </div>
        <div class="button-card" onclick="showContent('Statistics-and-Distribution')">
          <div class="title-b">Statistics and Distribution</div>
        </div>
        <div class="button-card" onclick="showContent('Web-Scoreboard')">
          <div class="title-b">Web Scoreboard</div>
        </div>
        <div class="button-card" onclick="showContent('Novel-View-Synthesis')">
          <div class="title-b">Novel View Synthesis</div>
        </div>
        <div class="button-card" onclick="showContent('3D-Shape-Reconstruction')">
          <div class="title-b">3D Shape Reconstruction</div>
        </div>
        <div class="button-card" onclick="showContent('References')">
          <div class="title-b">References</div>
        </div>
      </div>

      <div class="content-container" id="content-Preview">
        <br>
        <iframe src="./static/tables/preview.html" width="100%" height="1030" style="border:none;"></iframe>
        <br>
      </div>
      <div class="content-container" id="content-Statistics-and-Distribution">
        <br>
        <iframe src="./static/tables/table.html" width="100%" height="1030" style="border:none;"></iframe>
        <br>
      </div>
      <div class="content-container" id="content-Web-Scoreboard">
        <br>
        <iframe src="./static/tables/scoreboard-nerf.html" width="100%" height="760" style="border:none;"></iframe>
        <br>
        <iframe src="./static/tables/scoreboard-neus.html" width="100%" height="600" style="border:none;"></iframe>
        <br>
      </div>
      <div class="content-container" id="content-Novel-View-Synthesis">
        <br>
        <iframe src="./static/images/comparison-nerf-a.html" width="100%" height="1710" style="border:none;"></iframe>
      </div>
      <div class="content-container" id="content-3D-Shape-Reconstruction">
        <br>
        <iframe src="./static/images/comparison-neus.html" width="100%" height="1800" style="border:none;"></iframe>
        <br>
      </div>
      <div class="content-container" id="content-References">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <br>
              <h2 class="title is-3">References</h2>
              <p id="ref_1" class="content has-text-justified">
                [1] G. Oxholm and K. Nishino, “Multiview shape and reflectance from natural illumination,” in
                Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp.
                2155–2162.
                <br>[2] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale data for multiple-
                view stereopsis,” International Journal of Computer Vision, vol. 120, pp. 153–168, 2016.
                <br>[3] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples: Benchmarking large-scale
                scene reconstruction,” ACM Transactions on Graphics, vol. 36, no. 4, 2017.
                <br>[4] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi, R. Ng, and
                A. Kar, “Local light field fusion: Practical view synthesis with prescriptive sampling guidelines,”
                ACM Transactions on Graphics (TOG), vol. 38, no. 4, pp. 1–14, 2019.
                <br>[5] M. Li, Z. Zhou, Z. Wu, B. Shi, C. Diao, and P. Tan, “Multi-view photometric stereo: A robust
                solution and benchmark dataset for spatially varying isotropic materials,” IEEE Transactions on
                Image Processing, vol. 29, pp. 4159–4173, 2020.
                <br>[6] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. Lensch, “Nerd: Neural reflectance
                decomposition from image collections,” in Proceedings of the IEEE/CVF International Confer-
                ence on Computer Vision, 2021, pp. 12 684–12 694.
                <br>[7] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, “Ref-
                nerf: Structured view-dependent appearance for neural radiance fields,” in 2022 IEEE/CVF
                Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2022, pp. 5481–5490.
                <br>[8] Z. Kuang, K. Olszewski, M. Chai, Z. Huang, P. Achlioptas, and S. Tulyakov, “Neroic: Neural
                rendering of objects from online image collections,” ACM Trans. Graph., vol. 41, no. 4, jul
                2022. [Online]. Available: https://doi.org/10.1145/3528223.3530177
                <br>[9] M. Toschi, R. D. Matteo, R. Spezialetti, D. D. Gregorio, L. D. Stefano, and S. Salti,
                “Relight my nerf: A dataset for novel view synthesis and relighting of real world objects,”
                in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los
                Alamitos, CA, USA: IEEE Computer Society, jun 2023, pp. 20 762–20 772. [Online]. Available:
                https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.01989
                <br>[10] Z. Kuang, Y. Zhang, H.-X. Yu, S. Agarwala, S. Wu, and J. Wu, “Stanford-orb: A real-world 3d
                object inverse rendering benchmark,” 2023.
                <br>[11] T. Wu, J. Zhang, X. Fu, Y. Wang, L. P. Jiawei Ren, W. Wu, L. Yang, J. Wang, C. Qian,
                D. Lin, and Z. Liu, “Omniobject3d: Large-vocabulary 3d object dataset for realistic perception,
                reconstruction and generation,” in IEEE/CVF Conference on Computer Vision and Pattern
                Recognition (CVPR), 2023.
                <br>[12] Y. Liu, P. Wang, C. Lin, X. Long, J. Wang, L. Liu, T. Komura, and W. Wang, “Nero: Neural
                geometry and brdf reconstruction of reflective objects from multiview images,” in SIGGRAPH,
                2023.
                <br>[13] I. Liu, L. Chen, Z. Fu, L. Wu, H. Jin, Z. Li, C. M. R. Wong, Y. Xu, R. Ramamoorthi, Z. Xu
                et al., “Openillumination: A multi-illumination dataset for inverse rendering evaluation on real
                objects,” Advances in Neural Information Processing Systems, vol. 36, 2024.
                <br>[14] V. Jampani, K.-K. Maninis, A. Engelhardt, A. Karpur, K. Truong, K. Sargent, S. Popov,
                A. Araujo, R. Martin Brualla, K. Patel et al., “Navi: Category-agnostic image collections
                with high-quality 3d shape and pose annotations,” Advances in Neural Information Processing
                Systems, vol. 36, 2024.
                <br>[15] J. Shi, Y. Dong, H. Su, and S. X. Yu, “Learning non-lambertian object intrinsics across
                shapenet
                categories,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
                2017, pp. 1685–1694.
                <br>[16] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, “Blendedmvs: A
                large-scale dataset for generalized multi-view stereo networks,” Computer Vision and Pattern
                Recognition (CVPR), 2020.
                <br>[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf:
                Representing scenes as neural radiance fields for view synthesis,” Communications of the ACM,
                vol. 65, no. 1, pp. 99–106, 2021.
                <br>[18] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente,
                T. Dideriksen, H. Arora et al., “Abo: Dataset and benchmarks for real-world 3d object un-
                derstanding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                Recognition, 2022, pp. 21 126–21 136.
                <br>[19] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,
                A. Kembhavi, and A. Farhadi, “Objaverse: A universe of annotated 3d objects,” in Proceedings
                of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 142–
                13 153.
                <br>[20] Y.-C. Guo, “Instant neural surface reconstruction,” 2022, https://github.com/bennyguo/instant-
                nsr-pl.
                <br>[21] Y. Wang, Q. Han, M. Habermann, K. Daniilidis, C. Theobalt, and L. Liu, “Neus2: Fast learning
                of neural implicit surfaces for multi-view reconstruction,” in Proceedings of the IEEE/CVF
                International Conference on Computer Vision (ICCV), 2023.
                <br>[22] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3d gaussian splatting for real-time
                radiance field rendering,” ACM Transactions on Graphics, vol. 42, no. 4, July 2023. [Online].
                Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
                <br>
                <br>
              </p>
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in deep learning such as neural radiance fields and implicit neural representations have
              significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with
              <b>complex optical properties</b>, such as metals and glass, remains a formidable challenge due to their
              unique specular and light-transmission characteristics. To facilitate the development of solutions to
              these
              challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct
              materials—including <b>conductors</b>, <b>dielectrics</b>, <b>plastics</b>, <b>and their roughened
                variants</b>— and captured
              under 723 diverse lighting conditions.
            </p>
            <p>
              To this end, we utilized <b>physics-based rendering with laboratory-measured Indices of
                Refraction(IOR)</b> and
              generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides
              comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask.
            </p>
            <p>
              It stands as the first <b>large-scale</b> dataset enabling quantitative evaluations of existing
              algorithms on objects with diverse and challenging materials, thereby paving the way for the development
              of 3D reconstruction algorithms capable of <b>handling complex material properties</b>.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-centered is-full">
          <h2 class="title is-3">Statistics and Distribution</h2>
          <iframe src="./static/tables/table.html" width="100%" height="480" style="border:none;"></iframe>
          <h2 class="subtitle has-text-centered">
            <b>Comparison</b> with Existing Object-centric MVS Datasets.
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-centered is-full">
          <h2 class="title is-3">Novel View Synthesis</h2>
          <iframe src="./static/images/comparison-nerf-a.html" width="100%" height="1460" style="border:none;"></iframe>
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h2 class="title is-3">Novel View Synthesis</h2>
              <iframe src="./static/images/comparison-nerf.html" width="100%" height="700"
                style="border:none;"></iframe>
              <p id="ref_1" class="content has-text-centered">
                <br>Upper left is diffuse, upper right is plastic, lower left is conductor, lower right is dielectric.
              </p>
            </div>
          </div>
        </div>
      </section> -->

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="container is-centered is-full">
          <h2 class="title is-3">3D Shape Reconstruction</h2>
          <iframe src="./static/images/comparison-neus.html" width="100%" height="760" style="border:none;"></iframe>
          <h2 class="subtitle has-text-centered">
            <br>Upper left is <b>Diffuse</b>, upper right is <b>Plastic</b>, lower left is <b>Conductor</b>, lower right
            is <b>Dielectric</b>.
          </h2>
          </h2>
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">BibTeX</h2>
          <pre><code>@article{
              park2021nerfies,
              author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
              title     = {Nerfies: Deformable Neural Radiance Fields},
              journal   = {ICCV},
              year      = {2021},
              }</code></pre>
        </div>
      </div>
    </div>
  </section> -->
  <!-- 
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">References</h2>
          <p id="ref_1" class="content has-text-justified">
            <br>[1] G. Oxholm and K. Nishino, “Multiview shape and reflectance from natural illumination,” in
            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp.
            2155–2162.
            <br>[2] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale data for multiple-
            view stereopsis,” International Journal of Computer Vision, vol. 120, pp. 153–168, 2016.
            <br>[3] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples: Benchmarking large-scale
            scene reconstruction,” ACM Transactions on Graphics, vol. 36, no. 4, 2017.
            <br>[4] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi, R. Ng, and
            A. Kar, “Local light field fusion: Practical view synthesis with prescriptive sampling guidelines,”
            ACM Transactions on Graphics (TOG), vol. 38, no. 4, pp. 1–14, 2019.
            <br>[5] M. Li, Z. Zhou, Z. Wu, B. Shi, C. Diao, and P. Tan, “Multi-view photometric stereo: A robust
            solution and benchmark dataset for spatially varying isotropic materials,” IEEE Transactions on
            Image Processing, vol. 29, pp. 4159–4173, 2020.
            <br>[6] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. Lensch, “Nerd: Neural reflectance
            decomposition from image collections,” in Proceedings of the IEEE/CVF International Confer-
            ence on Computer Vision, 2021, pp. 12 684–12 694.
            <br>[7] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, “Ref-
            nerf: Structured view-dependent appearance for neural radiance fields,” in 2022 IEEE/CVF
            Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2022, pp. 5481–5490.
            <br>[8] Z. Kuang, K. Olszewski, M. Chai, Z. Huang, P. Achlioptas, and S. Tulyakov, “Neroic: Neural
            rendering of objects from online image collections,” ACM Trans. Graph., vol. 41, no. 4, jul
            2022. [Online]. Available: https://doi.org/10.1145/3528223.3530177
            <br>[9] M. Toschi, R. D. Matteo, R. Spezialetti, D. D. Gregorio, L. D. Stefano, and S. Salti,
            “Relight my nerf: A dataset for novel view synthesis and relighting of real world objects,”
            in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los
            Alamitos, CA, USA: IEEE Computer Society, jun 2023, pp. 20 762–20 772. [Online]. Available:
            https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.01989
            <br>[10] Z. Kuang, Y. Zhang, H.-X. Yu, S. Agarwala, S. Wu, and J. Wu, “Stanford-orb: A real-world 3d
            object inverse rendering benchmark,” 2023.
            <br>[11] T. Wu, J. Zhang, X. Fu, Y. Wang, L. P. Jiawei Ren, W. Wu, L. Yang, J. Wang, C. Qian,
            D. Lin, and Z. Liu, “Omniobject3d: Large-vocabulary 3d object dataset for realistic perception,
            reconstruction and generation,” in IEEE/CVF Conference on Computer Vision and Pattern
            Recognition (CVPR), 2023.
            <br>[12] Y. Liu, P. Wang, C. Lin, X. Long, J. Wang, L. Liu, T. Komura, and W. Wang, “Nero: Neural
            geometry and brdf reconstruction of reflective objects from multiview images,” in SIGGRAPH,
            2023.
            <br>[13] I. Liu, L. Chen, Z. Fu, L. Wu, H. Jin, Z. Li, C. M. R. Wong, Y. Xu, R. Ramamoorthi, Z. Xu
            et al., “Openillumination: A multi-illumination dataset for inverse rendering evaluation on real
            objects,” Advances in Neural Information Processing Systems, vol. 36, 2024.
            <br>[14] V. Jampani, K.-K. Maninis, A. Engelhardt, A. Karpur, K. Truong, K. Sargent, S. Popov,
            A. Araujo, R. Martin Brualla, K. Patel et al., “Navi: Category-agnostic image collections
            with high-quality 3d shape and pose annotations,” Advances in Neural Information Processing
            Systems, vol. 36, 2024.
            <br>[15] J. Shi, Y. Dong, H. Su, and S. X. Yu, “Learning non-lambertian object intrinsics across
            shapenet
            categories,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
            2017, pp. 1685–1694.
            <br>[16] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, “Blendedmvs: A
            large-scale dataset for generalized multi-view stereo networks,” Computer Vision and Pattern
            Recognition (CVPR), 2020.
            <br>[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf:
            Representing scenes as neural radiance fields for view synthesis,” Communications of the ACM,
            vol. 65, no. 1, pp. 99–106, 2021.
            <br>[18] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente,
            T. Dideriksen, H. Arora et al., “Abo: Dataset and benchmarks for real-world 3d object un-
            derstanding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
            Recognition, 2022, pp. 21 126–21 136.
            <br>[19] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,
            A. Kembhavi, and A. Farhadi, “Objaverse: A universe of annotated 3d objects,” in Proceedings
            of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 142–
            13 153.
            <br>[20] Y.-C. Guo, “Instant neural surface reconstruction,” 2022, https://github.com/bennyguo/instant-
            nsr-pl.
            <br>[21] Y. Wang, Q. Han, M. Habermann, K. Daniilidis, C. Theobalt, and L. Liu, “Neus2: Fast learning
            of neural implicit surfaces for multi-view reconstruction,” in Proceedings of the IEEE/CVF
            International Conference on Computer Vision (ICCV), 2023.
            <br>[22] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3d gaussian splatting for real-time
            radiance field rendering,” ACM Transactions on Graphics, vol. 42, no. 4, July 2023. [Online].
            Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
            <br>
          </p>
          <p class="auto-style1">
            <font color="#999999">Last update: July. 12, 2024</font>
          </p>
          </h2>
        </div>
      </div>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p class="auto-style1">
              <font color="#999999">Last update: June. 12, 2024</font>
            </p>
            <br>
            <p>
              Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
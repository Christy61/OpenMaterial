<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description" content="a novel dataset specifically designed for the radiance field reconstruction task.">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>OpenMaterial: Dataset about complex material for Neural Randiance Field</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;500&display=swap" rel="stylesheet">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OpenMaterial: Dataset about complex material for Neural Randiance
              Field</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=o8BdwuMAAAAJ">Zheng
                  Dang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href=" ">Jialu Huang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=LKPpmXQAAAAJ&hl=zh-CN&oi=ao">Fei
                  Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>xxx University</span>
              <span class="author-block"><sup>2</sup>xxx University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="https://arxiv.org" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="https://arxiv.org" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block"> -->
                <!-- <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark"> -->
                <!-- <a href="https://www.youtube.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <!-- <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="https://github.com/Dangzheng/Benchmarking_Everything"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://www.dropbox.com/scl/fo/rjlgh7jp44atmvb78zw4h/AOTURMe0vAv6O153vpb_qUg?rlkey=fz88na3k6wnw0yylki6401da8&e=1&dl=0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/teaser.png" style="width:80%; margin-left:auto; margin-right:auto; display:block">
        <h2 class="subtitle has-text-centered">
          <span class="dnerf"><b>OpenMaterial</b></span> Qualitative result of the material result.
        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce OpenMaterial, a novel dataset specifically designed for the radiance field reconstruction
              task.
            </p>
            <p>
              Recent advances
              in radiance field recontruction task have enabled a wide range of applications
              in various fields. While these applications continue to see improvements, there
              currently exists no benchmark capable of effectively assessing the performance
              of different radiance field reconstruction methods across various <b>reflective and
                refractive surfaces</b>, which are influenced by material differences. Existing datasets
              typically consist of objects made from mixed materials, which are inadequate for
              isolating and evaluating differences caused by individual materials alone.
            </p>
            <p>
              OpenMaterial addresses this gap by providing a comprehensive set of data that focuses on
              <b>singular-material objects</b>, thereby facilitating more precise evaluations of material-based
              variations in radiance field reconstructions. Using this dataset, we establish
              the fist comprehensive material...
            </p>
          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h2 class="title is-3">Statistics and Distribution</h2>
              <iframe src="./static/tables/table.html" width="100%" height="480" style="border:none;"></iframe>
              </h2>
              <h2 class="subtitle has-text-centered">
                <b>Comparison</b> with Existing Object-centric MVS Datasets.
              </h2>
            </div>
            <!--/ Abstract. -->

            <!-- Paper video. -->
            <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
            <!--/ Paper video. -->
          </div>
      </section>


      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h2 class="title is-3">Novel View Synthesis</h2>
              <iframe src="./static/images/comparison-nerf.html" width="100%" height="700"
                style="border:none;"></iframe>
              </h2>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="container is-max-desktop">
              <h2 class="title is-3">3D Shape Reconstruction</h2>
              <iframe src="./static/images/comparison-neus.html" width="100%" height="700"
                style="border:none;"></iframe>
              <p id="ref_1" class="content has-text-centered">
                <br>Upper left is diffuse, upper right is plastic, lower left is conductor, lower right is dielectric.
              </p>
              </h2>
            </div>
          </div>
        </div>
      </section>

      <section class="section" id="BibTeX">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title">BibTeX</h2>
              <pre><code>@article{
              <!-- park2021nerfies,
              author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
              title     = {Nerfies: Deformable Neural Radiance Fields},
              journal   = {ICCV},
              year      = {2021}, -->
              }</code></pre>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">References</h2>
              <p id="ref_1" class="content has-text-justified">
                <br>[1] G. Oxholm and K. Nishino, “Multiview shape and reflectance from natural illumination,” in
                Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp.
                2155–2162.
                <br>[2] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale data for multiple-
                view stereopsis,” International Journal of Computer Vision, vol. 120, pp. 153–168, 2016.
                <br>[3] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples: Benchmarking large-scale
                scene reconstruction,” ACM Transactions on Graphics, vol. 36, no. 4, 2017.
                <br>[4] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi, R. Ng, and
                A. Kar, “Local light field fusion: Practical view synthesis with prescriptive sampling guidelines,”
                ACM Transactions on Graphics (TOG), vol. 38, no. 4, pp. 1–14, 2019.
                <br>[5] M. Li, Z. Zhou, Z. Wu, B. Shi, C. Diao, and P. Tan, “Multi-view photometric stereo: A robust
                solution and benchmark dataset for spatially varying isotropic materials,” IEEE Transactions on
                Image Processing, vol. 29, pp. 4159–4173, 2020.
                <br>[6] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. Lensch, “Nerd: Neural reflectance
                decomposition from image collections,” in Proceedings of the IEEE/CVF International Confer-
                ence on Computer Vision, 2021, pp. 12 684–12 694.
                <br>[7] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, “Ref-
                nerf: Structured view-dependent appearance for neural radiance fields,” in 2022 IEEE/CVF
                Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2022, pp. 5481–5490.
                <br>[8] Z. Kuang, K. Olszewski, M. Chai, Z. Huang, P. Achlioptas, and S. Tulyakov, “Neroic: Neural
                rendering of objects from online image collections,” ACM Trans. Graph., vol. 41, no. 4, jul
                2022. [Online]. Available: https://doi.org/10.1145/3528223.3530177
                <br>[9] M. Toschi, R. D. Matteo, R. Spezialetti, D. D. Gregorio, L. D. Stefano, and S. Salti,
                “Relight my nerf: A dataset for novel view synthesis and relighting of real world objects,”
                in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los
                Alamitos, CA, USA: IEEE Computer Society, jun 2023, pp. 20 762–20 772. [Online]. Available:
                https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.01989
                <br>[10] Z. Kuang, Y. Zhang, H.-X. Yu, S. Agarwala, S. Wu, and J. Wu, “Stanford-orb: A real-world 3d
                object inverse rendering benchmark,” 2023.
                <br>[11] T. Wu, J. Zhang, X. Fu, Y. Wang, L. P. Jiawei Ren, W. Wu, L. Yang, J. Wang, C. Qian,
                D. Lin, and Z. Liu, “Omniobject3d: Large-vocabulary 3d object dataset for realistic perception,
                reconstruction and generation,” in IEEE/CVF Conference on Computer Vision and Pattern
                Recognition (CVPR), 2023.
                <br>[12] Y. Liu, P. Wang, C. Lin, X. Long, J. Wang, L. Liu, T. Komura, and W. Wang, “Nero: Neural
                geometry and brdf reconstruction of reflective objects from multiview images,” in SIGGRAPH,
                2023.
                <br>[13] I. Liu, L. Chen, Z. Fu, L. Wu, H. Jin, Z. Li, C. M. R. Wong, Y. Xu, R. Ramamoorthi, Z. Xu
                et al., “Openillumination: A multi-illumination dataset for inverse rendering evaluation on real
                objects,” Advances in Neural Information Processing Systems, vol. 36, 2024.
                <br>[14] V. Jampani, K.-K. Maninis, A. Engelhardt, A. Karpur, K. Truong, K. Sargent, S. Popov,
                A. Araujo, R. Martin Brualla, K. Patel et al., “Navi: Category-agnostic image collections
                with high-quality 3d shape and pose annotations,” Advances in Neural Information Processing
                Systems, vol. 36, 2024.
                <br>[15] J. Shi, Y. Dong, H. Su, and S. X. Yu, “Learning non-lambertian object intrinsics across
                shapenet
                categories,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
                2017, pp. 1685–1694.
                <br>[16] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, “Blendedmvs: A
                large-scale dataset for generalized multi-view stereo networks,” Computer Vision and Pattern
                Recognition (CVPR), 2020.
                <br>[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf:
                Representing scenes as neural radiance fields for view synthesis,” Communications of the ACM,
                vol. 65, no. 1, pp. 99–106, 2021.
                <br>[18] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente,
                T. Dideriksen, H. Arora et al., “Abo: Dataset and benchmarks for real-world 3d object un-
                derstanding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                Recognition, 2022, pp. 21 126–21 136.
                <br>[19] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,
                A. Kembhavi, and A. Farhadi, “Objaverse: A universe of annotated 3d objects,” in Proceedings
                of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 142–
                13 153.
                <br>[20] Y.-C. Guo, “Instant neural surface reconstruction,” 2022, https://github.com/bennyguo/instant-
                nsr-pl.
                <br>[21] Y. Wang, Q. Han, M. Habermann, K. Daniilidis, C. Theobalt, and L. Liu, “Neus2: Fast learning
                of neural implicit surfaces for multi-view reconstruction,” in Proceedings of the IEEE/CVF
                International Conference on Computer Vision (ICCV), 2023.
                <br>[22] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3d gaussian splatting for real-time
                radiance field rendering,” ACM Transactions on Graphics, vol. 42, no. 4, July 2023. [Online].
                Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
                <br>
              </p>
              <p class="auto-style1">
                <font color="#999999">Last update: May. 17, 2024</font>
              </p>
              </h2>
            </div>
          </div>
        </div>
      </section>

      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content has-text-centered">
                <p>
                  Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>


</body>

</html>